<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in generated images.">
  <meta name="keywords" content="FineGRAIN, Text-to-Image, Vision Language Models, Evaluation, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://goldstein-lab.umd.edu">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://goldstein-lab.umd.edu">
            Goldstein Lab
          </a>
          <a class="navbar-item" href="https://umd.edu">
            University of Maryland
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Kevin David Hayes</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Micah Goldblum</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="#">Gowthami Somepalli</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Vikash Sehwag</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="#">Ashwinee Panda</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Tom Goldstein</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland,</span>
            <span class="author-block"><sup>2</sup>Columbia University,</span>
            <span class="author-block"><sup>3</sup>Sony AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2024.XXXXX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2024.XXXXX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/khayes95/FineGRAIN_Eval"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/KevinDavidHayes/t2i-finegrain"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/figures/pipeline_overview.png" alt="FineGRAIN Overview" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">FineGRAIN</span> provides a structured evaluation framework for both Text-to-Image models and Vision Language Models using 27 specific failure modes.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-counting">
          <img src="./static/images/examples/counts.png" alt="Counting failure example" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Counts/Multiple Objects:</strong> Models struggle with generating exact numbers</p>
        </div>
        <div class="item item-text">
          <img src="./static/images/long_text_specific_0.png" alt="Text rendering failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Long Text Rendering:</strong> Accurate text generation remains challenging</p>
        </div>
        <div class="item item-anatomy">
          <img src="./static/images/examples/human_action_0.png" alt="Anatomical failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Anatomical Accuracy:</strong> Human limb and torso representation errors</p>
        </div>
        <div class="item item-spatial">
          <img src="./static/images/spatial_failure.png" alt="Spatial relations failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Spatial Relations:</strong> Difficulty with object positioning and relationships</p>
        </div>
        <div class="item item-color">
          <img src="./static/images/color_failure.png" alt="Color attribute failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Color Binding:</strong> Incorrect color-object associations</p>
        </div>
        <div class="item item-physics">
          <img src="./static/images/physics_failure.png" alt="Physics failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Physics:</strong> Violations of physical laws and realistic interactions</p>
        </div>
        <div class="item item-negation">
          <img src="./static/images/negation_failure.png" alt="Negation failure" style="width: 100%; height: 300px; object-fit: cover;">
          <p class="has-text-centered" style="margin-top: 10px;"><strong>Negation:</strong> Difficulty understanding "without" or "not" instructions</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models.
          </p>
          <p>
            We propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our dataset includes prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and corresponding annotations from a VLM (Molmo) annotated by an LLM (Llama3).
          </p>
          <p>
            By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Framework Overview. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">The FineGRAIN Framework</h2>
          <p>
            FineGRAIN is an agentic system for rating Text-to-Image and Image-to-Text models by determining whether a VLM can identify anything wrong with an image generated by a T2I model. The framework provides boolean scoring, objective annotations, and interpretable results.
          </p>
          <img src="./static/images/finegrain_framework.png" alt="FineGRAIN Framework Architecture" style="width: 100%; height: auto;">
        </div>
      </div>
      <!--/ Framework Overview. -->

      <!-- Failure Modes. -->
      <!-- Failure Modes Interactive Table -->
      <div class="column">
        <h2 class="title is-3">27 Failure Modes - Interactive Explorer</h2>
        
        <div class="filter-controls" style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
          <div class="columns">
            <div class="column">
              <div class="field">
                <label class="label">Search Failure Modes</label>
                <div class="control has-icons-left">
                  <input class="input" type="text" id="searchInput" placeholder="Search by name, description, or prompt...">
                  <span class="icon is-small is-left">
                    <i class="fas fa-search"></i>
                  </span>
                </div>
              </div>
            </div>
            <div class="column">
              <div class="field">
                <label class="label">Filter by Difficulty</label>
                <div class="control">
                  <div class="select">
                    <select id="difficultyFilter">
                      <option value="all">All Difficulties</option>
                      <option value="high">High Failure Rate (>70%)</option>
                      <option value="medium">Medium Failure Rate (30-70%)</option>
                      <option value="low">Low Failure Rate (<30%)</option>
                    </select>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        
        <div class="table-container" style="box-shadow: 0 2px 8px rgba(0,0,0,0.1); border-radius: 8px; overflow: hidden;">
          <table class="table is-fullwidth is-hoverable">
            <thead>
              <tr style="background-color: #f8f9fa;">
                <th class="sortable" data-sort="name" style="cursor: pointer;">
                  Failure Mode
                  <i class="fas fa-sort" id="sort-name"></i>
                </th>
                <th class="sortable" data-sort="rate" style="cursor: pointer;">
                  Failure Rate
                  <i class="fas fa-sort-down" id="sort-rate"></i>
                </th>
                <th>Description</th>
                <th>Sample Prompt</th>
              </tr>
            </thead>
            <tbody id="failureModeTable">
              <!-- Table rows populated by JavaScript -->
            </tbody>
          </table>
        </div>
        
        <div class="notification is-info is-light" style="margin-top: 20px;">
          <p><strong>How to read this table:</strong></p>
          <ul>
            <li><strong>Failure Rate:</strong> Percentage of images that contain the failure mode</li>
            <li><strong>Sample Prompts:</strong> Real prompts from our evaluation</li>
            <li><strong>Color coding:</strong> Red (>70%) = Very challenging, Yellow (30-70%) = Moderate, Green (<30%) = Less challenging</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Failure Modes. -->

    <!-- Model Performance. -->
    <div class="columns is-centered" id="leaderboard">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Performance Leaderboard</h2>

        <div class="content has-text-justified">
          <p>
            Performance comparison across 5 T2I models on 27 failure modes. Scores represent success rates as judged by humans. We immediately see the benefits of taking a fine-grained approach - some categories that were simply lumped together actually have very different success rates.
          </p>
        </div>
        
        <div class="content has-text-centered">
          <img src="./static/images/figures/t2i_model_compare.png" alt="T2I Model Performance Comparison" style="width: 100%; height: auto; max-width: 800px;">
        </div>

        <h3 class="title is-4">Key Findings</h3>
        <div class="content has-text-justified">
          <p>
            All models fail completely to generate correct "Counts or Multiple Objects", revealing a systematic weakness in numerical reasoning. For example, "Six oval stones" could receive a poor score because it had the wrong number of stones OR because the stones were not oval - our evaluation separates these failure modes.
          </p>
        </div>

      </div>
    </div>
    <!--/ Model Performance. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">FineGRAIN vs. VQAScore</h2>
    
    <div class="columns is-centered">
      <div class="column is-two-thirds">
        <div class="content has-text-justified">
          <p>
            FineGRAIN achieves <strong>67.4% agreement</strong> with human annotations compared to <strong>57.7%</strong> for VQAScore - a significant 10% improvement in accuracy. While FineGRAIN achieves near-human performance for objective categories like "Counts or Multiple Objects" and "Long text specific", it performs less well on subjective categories like "Surreal."
          </p>
        </div>
        
        <div class="content has-text-centered">
          <img src="./static/images/figures/agreement_comparison.pdf" alt="Agreement Comparison Chart" style="width: 100%; height: auto;">
        </div>
      </div>
      
      <div class="column">
        <div class="content">
          <h3 class="title is-4">Key Advantages</h3>
          <div class="notification is-info is-light">
            <p><strong>Boolean Scoring:</strong> Clear pass/fail determination for deployment pipelines</p>
          </div>
          <div class="notification is-success is-light">
            <p><strong>Objective Focus:</strong> Emphasis on unambiguous criteria like counting and text accuracy</p>
          </div>
          <div class="notification is-warning is-light">
            <p><strong>Interpretable Results:</strong> LLM judge provides reasoning for each evaluation</p>
          </div>
        </div>
      </div>
    </div>

    <div class="content has-text-justified">
      <p>
        <strong>VLMs Cannot Be Trusted:</strong> One finding that sharply differs from prior work is that we give all models very low scores for counting. This is because prior work has been lax at assessing whether models generate the exact right number of objects. VQAScore relies on the VLM to correctly determine whether "3 bananas" is in the image, but we show this approach is fundamentally flawed.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Prompt Difficulty Scaling</h2>
    
    <div class="content has-text-justified">
      <p>
        A unique advantage of FineGRAIN is the ability to adjust evaluation difficulty programmatically. We demonstrate this with two failure modes: generating long text and counting multiple objects. Even the best-performing models have significant room for improvement as difficulty scales.
      </p>
    </div>

    <div class="columns">
      <div class="column">
        <h3 class="title is-4">Text Length Scaling</h3>
        <p>Performance drops dramatically as text length increases. Models that handle short phrases fail completely on paragraph-length text generation.</p>
        <img src="./static/images/ablation/text_token_ablation.jpg" alt="Text Length Performance" style="width: 100%; height: auto;">
      </div>
      
      <div class="column">
        <h3 class="title is-4">Object Count Scaling</h3>
        <p>Success rates plummet when asked to generate more than 3-4 objects. SDXL's counting performance drops sharply beyond very small numbers.</p>
        <img src="./static/images/ablation/counting_distinct_model.jpg" alt="Counting Performance" style="width: 100%; height: auto;">
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hayes2024finegrain,
  author    = {Hayes, Kevin David and Goldblum, Micah and Somepalli, Gowthami and Sehwag, Vikash and Panda, Ashwinee and Goldstein, Tom},
  title     = {FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges},
  journal   = {arXiv preprint arXiv:2024.XXXXX},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/papers/finegrain_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/finegrain-ai" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website template is based on the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project page</a>. We thank the authors for making their template available.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>